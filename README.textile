h1. Adventures in Data - Looking up from the Mezzanine of ETL

Being a programmer in the 21st century you've been exposed to databases and the transfer of data both out of (the easy part) and into (not as easy as we think) our applications.

I was exposed to the world of ETL during my time at a Philladelphia area data integration company.  We had to deal with a wide variety of file formats and relational data models.

{{{ need more references }}} 
ETL (Extract, Transform, Load) is a blanket term for the processes and tools used to export data from one system and import it into another.

This talk is about my experiences with all those files, formats and processing them en masse.  The ways in which we diverged a bit from the standard model of ETL and some of the tools and approaches we used to wrangle all that data.


h1. Aaron's ETL Outline

* What is ETL
  ETL (Extract, Transform, Load) is the blanket term for the tools and processes used to export data from one system and import them into antoher.  This typicall manifests itself as a dump of the source system's data base, a process to make the data model and values compatible with a target system and an import process to get the data into the target system.

  There are challgenges in data model incongruencies, the formatting of values, the proper identification of data and types, the accuracy and consisteny of the export from the source system, and more. {{{ what else? }}}

    Relational databases often require closely tracked foreign Key {re}construction during the import process.  This leads to multi-step ETL processes where the initial data is bulk imported, then primary entites enumerated, then multiple rounds of join / updates are performed to propigate the primary keys.  Once all the data has been staged and stitched together in this way, then the final table imports are performed -- or constraints are re-enabled and indexes rebuilt.

    We found that that traditional methodology lead to a high degree of analysis before hand (to ensure we understood the data, how to model it and how to transform it), a complex to build and complex to maintain ETL process (especially in the face of input sources that would change format over time, and a high volume of different data sets).

    We found that traditional ETL approaches and processing lead us into expending a lot of effort on each ETL mapping, opaque processes that did not provide good feedback about progress, brittle processes that did not tolerate data inconsistencies well (often enough to handle a handful of errors, either an entire process needed to be restarted from early on, or the correction of that handful had to be done as a one-off that was not able to effectively be made into a formalized part of the ETL processing - often leading to manual process being instutionalized into the ETL processing).

    {{{ Need to expand on this more }}} The way we did consolidation was also something that fed into our choices here.

  Oracle's SQL Loader, Mysql's 'COPY INFILE', MS Access's import features.

* ETL @ HMS
** what type of problem was being solved?
** data volume
** data source type
** speed ETL
** frequency of ETL
** technique or tool(s) used for ETL
** describe the evolution of the current tool(s)
*** what did you guys used to do?
*** how did you guys arrive at the current approach?
** how long did it take to develop the initial version of the tool(s)
** who were the end users of the tool(s)
* Exploratory ETL process via configuration
** ability to configure during runtime
** visually inspect data while it is being processed
* Abstract Table Tools
** How do they work
** What data sources could they handle (source, and destination)
** Syntax
** How is transformation being handled?
** How does it handle the following scenarios:
*** one-to-one (trivial)
*** one-to-many - one record becomes multiple records
*** many-to-many - one logical record from multiple tables into multiple tables
* Tools can be chained as needed

1. The era of table stores
2. Relational Databases
3. Reporting Systems Co-opted as Export tools
4. Import / Export tools
5. Relational Data Model Mapping
6. There is no Spoon : How to stop worrying and Love Ontological Mapping Techniques

h1. Outline

h2. What is ETL?

Extract, Transform, Load

The goal is to take a data set from one system and load it into another system where there are likely to be significant differences in how the two systems store, model and otherwise represent their data.

Extract data from a system (be they text files, a database dump, or other propreitary format), transform them into a form which can be understood (loaded) into the target system (change the encoding, data types, standardize values, morph the data model).

Most of this talk is going to focus on the movement of relationaly modeled data, tables of records with foreign key relationships, though ETL does not necessarily imply relationally modeled data.  Many of the tools that will be discussed were designed specifically to deal with this type of data.

h2. My own Experience with Data Integration

My programming career started at a 401k benefits managment company, my job was to take a flat-file export from a Unisys main-frame and load it into an Integrated Voice Response system where customers could use a phone to check their balances and change their allocations.

Little did I know at the time of what was to come...

More recently I worked for a company that did larger scale data integration.  At that company we were integrating thousands of independent data sources into a single consolidated master file of data.  These were acquired in a wide variety of ways: assets purchased from companies that had gone out of business, public domain sources, government agencies, other data providers and even our own customers in reciprocial agreements.

In nearly all cases we had very little influence on how we received the data, most often the sending party did not have the resources or expertise, or we did not wish to pay the preimium, to have them adapt their export processes to our input format or data model.

Early on we didn't have a data model anyway.

We needed to load tens to hundreds of sources a day, where a source was a snapshot of an input database.  Some of these were to be incorporeated into the master file, some were customer data integration jobs - to enhance their data and produce a deliverable back.

We were a start-up and could not afford to purchase expensive ETL software, nor could we afford to staff up a large number of highly skilled (and expensive) database developers to perform the data transformation and loading process.

What we ended up doing was thinking a bit differently about the different facets of ETL and creating tools that allowed us to leverage technical anlysts to perform the ETL mapping, without requiring programming skills, or significant amounts of training.

We even saw a not insignificant amount of data errors or corruption - we had optical media from defunct companies that had bit level errors.  The data was valueable enought that we worked to correct and accomodate those kinds of errors in our system.

h3. Data Volumes

We dealt with tens to hundreds of database snapshots a day.  These sources would be up to several million records each.  The entire system maintained billions of records of data, connected to about five million entities.

h3. Data Formats

* Text Formats:
** Structured
*** Fixed Width
**** Record Separators optional
**** Padding
**** Often Not Self-describing (no header row)
**** Packed Fields, more than 1 value in a field (delimited within fixed width)
**** Mis-fielded Data
**** 'Somewhat fixed' - data overrunning into adjacent fields
*** Delimited
**** Name for the 'well formed delimted' like tab/pipe?
**** Name for the 'complex delimited' like CSV?
** SemiStructured
*** PDF Files
*** HTML Pages
*** Legal Proceedings

h2. Start with the lowest level, move up one conceptual level at a time.

* Files: Blobs of Binary Data Sitting on Disk
* Records: Delimited or Fixed Width
* Feilds: Simple Delimited, Complex Delimited, Fixedwidth
* Model: types, validation and meaning from a text dump


* Files: Blobs of Binary Data Sitting on Disk
** Encoding: Streams of Characters
  By far the most common format I had to deal with was basic ascii represented as 8-bit characters, which
  is in fact, equivalent to Latin-X.  The next most common was Latin-15 (ISO-8859-15), allowing for other
  common western language's extended character sets.
*** ASCII
*** utf-8
*** Latin-X
*** EBCDIC
** First recognition of structure: lines or records (record delmiter / fixed record size)
*** record delimiters are not consistent
*** record delimiters are embedded in the data
*** sometimes there are _no_ record separators
** Second recognition of structure: fields / columns
*** Headers may not be present
**** this means you have to specify them as part of the descriptor for the source
** Keys
*** Surprise!: keys
*** sometimes not present (!)
*** sometimes documentation is 'wrong'
**** specify the key they used
**** but file is a de-normalized dump of primary source
**** key no longer unique
**** complicates your import
**** simple analyiss tricks for determining the uniqueness of a key column
*** Simple analysis tricks and tools for getting a feel for the data
**** cut | sort | uniq -c | wc -l
**** cut | sort | uniq -c | sort -nr
**** abtab_view
**** abtab_historgram

* Developing a Taxonomy and A Descriptor Schema For Files that represent Tables
** One Column must become Many (hard)
** Many Columns must become One (easier)
** Data type must change:
*** numeric: fairly straightforward
*** enumerated values
**** coding tables
**** did the vendor supply one?
**** does their data actualy match the documentation? (extra / missing / unused values?)
**** strategies for recognizing what's present
**** strategies for handling the mapping
**** strategies for modeling your own standardized lookup tables
***** be as specific as possible, things are much easier on you if your LU tables are a super-set of all your inputs
***** or if not, in addition to mapping, pass-through the original data for re-analysis later
** Computed Values, precision, accuracy
*** searching to determining the min/max/avg precision
*** currency
*** lon/lat

* Data Models
** Each file is a table
** Join key as single column
** Join key as tuple (multiple columns)
** Join key as computed composite
** Suprise!: 1-1 table relations based on record position (ugh)

* The great tragedy : loss of data
** as you transform, be prescient of what you discard
** you can't get it back
** if space is cheap for you, carry forward, or better yet deferr

* Consolidation


h2. Talk about how much data we used to process

* >2k+ Different Relational Data Models
* Tens of Thousands of Snapshots
* billions of records
* all for ~5 million 'entities'
